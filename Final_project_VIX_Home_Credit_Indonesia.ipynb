{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZLG5G3gtlv0k",
        "ZrSl7bSSFVlR",
        "NvYvjoAbXNA3",
        "Da59GkCaliZX",
        "en_ir8mcl_PZ",
        "4AcHOUPefjhV",
        "5pWJsPjajYK_",
        "U53ft9O0m4B3",
        "Q1lqD1N9XJ6T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDvQzzhqSDrc",
        "outputId": "d24643b6-74b8-4f7e-defc-3a97db5020dc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.8/dist-packages (2.5.1.post0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.21.6)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.12.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "93C71KGCR9Pm"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import sys\n",
        "import csv\n",
        "import datetime\n",
        "import operator\n",
        "import joblib\n",
        "import gc\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from collections import OrderedDict\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from scipy.stats import norm, skew, probplot\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from category_encoders.target_encoder import TargetEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option(\"display.max_columns\", 200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHIEwiEdSHeR",
        "outputId": "1651f944-9dc8-4952-c780-6dcad228988b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/sample_submission.csv'\n",
        "bureau_data = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/bureau.csv'\n",
        "bureau_balance = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/bureau_balance.csv'\n",
        "pc_balance = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/POS_CASH_balance.csv'\n",
        "test = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/application_test.csv'\n",
        "train = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/application_train.csv'\n",
        "cc_balance = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/credit_card_balance.csv'\n",
        "install_balance = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/installments_payments.csv'\n",
        "prev_app = '/content/drive/MyDrive/Project DS/Home-Credit-Indonesia/final project/dataset/previous_application.csv'"
      ],
      "metadata": {
        "id": "7f2riSNoSLvP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "ZLG5G3gtlv0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            \n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "mP4e8i6WECe3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_nan(dataframe):\n",
        "  print('The shape:', dataframe.shape)\n",
        "  result = pd.concat([dataframe.isnull().sum().reset_index(name ='sum_of_nulls'), \n",
        "                      dataframe.dtypes.reset_index(name=\"dtypes\")], axis=1)\\\n",
        "                      .T.drop_duplicates()\\\n",
        "                      .T.sort_values('sum_of_nulls', ascending=False)\n",
        "  return result"
      ],
      "metadata": {
        "id": "8zJeiMuaFwCO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n",
        "    \"\"\"Create a new column for each categorical value in categorical columns. \"\"\"\n",
        "    original_columns = list(df.columns)\n",
        "    if not categorical_columns:\n",
        "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
        "    categorical_columns = [c for c in df.columns if c not in original_columns]\n",
        "\n",
        "    del original_columns; gc.collect()\n",
        "    return df, categorical_columns"
      ],
      "metadata": {
        "id": "OZWdJtv4HJX-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
        "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
        "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
        "                               for e in agg_df.columns.tolist()])\n",
        "    return agg_df.reset_index()"
      ],
      "metadata": {
        "id": "hMs2mbnLHdCJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
        "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
        "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)"
      ],
      "metadata": {
        "id": "R6BHgGG9HUxW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Korelasyonlar\n",
        "def high_correlation(data, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7):\n",
        "    if len(remove) > 0:\n",
        "        cols = [x for x in data.columns if (x not in remove)]\n",
        "        c = data[cols].corr(method=corr_coef)\n",
        "    else:\n",
        "        c = data.corr(method=corr_coef)\n",
        "\n",
        "    for i in c.columns:\n",
        "        cr = c.loc[i].loc[(c.loc[i] >= corr_value) | (c.loc[i] <= -corr_value)].drop(i)\n",
        "        if len(cr) > 0:\n",
        "            print(i)\n",
        "            print(\"-------------------------------\")\n",
        "            print(cr.sort_values(ascending=False))\n",
        "            print(\"\\n\")"
      ],
      "metadata": {
        "id": "Kt9rp6ZkgWi4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rare Encoding\n",
        "def rare_encoder(dataframe, rare_perc, cat_cols):\n",
        "   \n",
        "    rare_columns = [col for col in cat_cols if\n",
        "                    (dataframe[col].value_counts() / len(dataframe) < rare_perc).sum() > 1]\n",
        "\n",
        "    for col in rare_columns:\n",
        "        tmp = dataframe[col].value_counts() / len(dataframe)\n",
        "        rare_labels = tmp[tmp < rare_perc].index\n",
        "        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n",
        "\n",
        "    return dataframe"
      ],
      "metadata": {
        "id": "O3qeaboXgsVY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing Bureu"
      ],
      "metadata": {
        "id": "ZrSl7bSSFVlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bureau_balance(path, num_rows= None):\n",
        "    bb = pd.read_csv(path, nrows= num_rows)\n",
        "    bb, categorical_cols = one_hot_encoder(bb, nan_as_category= False)\n",
        "    \n",
        "    # Calculate rate for each category with decay\n",
        "    bb_processed_cat = bb.groupby('SK_ID_BUREAU')[categorical_cols].mean().reset_index()\n",
        "    bb_processed_num = bb.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].size().reset_index()\n",
        "    \n",
        "    bb_processed = bb_processed_cat.merge(bb_processed_num, how='left', on='SK_ID_BUREAU')\n",
        "\n",
        "    bb_processed.rename(columns = {'MONTHS_BALANCE':'MONTHS_BALANCE_SIZE_BUREAU_BALANCE'}, inplace = True)\n",
        "    # Min, Max, Count and mean duration of payments (months)\n",
        "    #agg = {'MONTHS_BALANCE': ['min', 'max', 'mean', 'size']}\n",
        "   # bb_processed = group_and_merge(bb, bb_processed, '', agg, 'SK_ID_BUREAU')\n",
        "    del bb; gc.collect()\n",
        "    return bb_processed"
      ],
      "metadata": {
        "id": "PjBeSf67GhDh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bureau(path, num_rows= None, cat_cols = False):\n",
        "    \"\"\" Process bureau.csv and bureau_balance.csv and return a pandas dataframe. \"\"\"\n",
        "    bureau = pd.read_csv(path, nrows= num_rows)\n",
        "  \n",
        "    # Handling NaN\n",
        "    bureau = bureau.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
        "    bureau = bureau.apply(lambda x: x.fillna(x.mode()[0]) if x.dtype == \"O\" else x, axis=0)\n",
        "\n",
        "    # Credit duration and credit/account end date difference\n",
        "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
        "    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
        "    \n",
        "    # Credit to debt ratio and difference\n",
        "    bureau['CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / bureau['AMT_CREDIT_SUM']\n",
        "   # bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
        "    bureau['ANNUITY_TO_CREDIT_RATIO'] =   bureau['AMT_ANNUITY'] / bureau['AMT_CREDIT_SUM']\n",
        "\n",
        "    # Fillna to zero\n",
        "    bureau.fillna(0, inplace=True)\n",
        "    # One-hot encoder\n",
        "    bureau, categorical_cols = one_hot_encoder(bureau, nan_as_category= False)\n",
        "\n",
        "    #bb_process = bureau.groupby('SK_ID_CURR')[categorical_cols].mean().reset_index()\n",
        "    # Join bureau balance features\n",
        "    #bureau = bureau.merge(get_bureau_balance(bureau_balance, num_rows), how='left', on='SK_ID_BUREAU')\n",
        "\n",
        "    if cat_cols == True:\n",
        "      return bureau, categorical_cols\n",
        "\n",
        "    del categorical_cols; gc.collect()\n",
        "    \n",
        "    return bureau\n"
      ],
      "metadata": {
        "id": "oD0EGn-KME2i"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_bureau():\n",
        "  bureau, cols = get_bureau(bureau_data, cat_cols=True)\n",
        "  bb = get_bureau_balance(bureau_balance)\n",
        "\n",
        "  # Join bureau balance features\n",
        "  result = bureau.merge(bb, how='left', on='SK_ID_BUREAU').fillna(0)\n",
        "  #bb_process = result.groupby('SK_ID_CURR')[cols].mean().reset_index()\n",
        "  metrics = [\"CREDIT_DURATION\",\"ENDDATE_DIF\", 'CREDIT_DEBT_RATIO', 'ANNUITY_TO_CREDIT_RATIO']\n",
        "  columns = list(cols) + bb.columns.tolist() +  metrics + [\"SK_ID_CURR\"] \n",
        "  result = result[columns]\n",
        "  result = pd.pivot_table(result, index='SK_ID_CURR').reset_index()\n",
        "\n",
        "  del bureau, cols, bb, columns, metrics; gc.collect()\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "0U7kn4U2cnLR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing Credit Card Balance"
      ],
      "metadata": {
        "id": "NvYvjoAbXNA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_credit_balance(num_rows=None):\n",
        "  cc = pd.read_csv(cc_balance, nrows = num_rows)\n",
        "  cc = cc[cc[\"AMT_DRAWINGS_ATM_CURRENT\"] > 0]\n",
        "  # Handling NaN\n",
        "  amt_col = [\"AMT_PAYMENT_CURRENT\", \"AMT_DRAWINGS_ATM_CURRENT\", \"AMT_INST_MIN_REGULARITY\",\n",
        "             \"AMT_DRAWINGS_OTHER_CURRENT\", \"AMT_DRAWINGS_POS_CURRENT\"]\n",
        "  for col in amt_col:\n",
        "    cc[col] = cc[col].fillna(cc[col].median())\n",
        "    cc[col + \"_TO_AMT_BALANCE\"] = cc[col] / cc[\"AMT_BALANCE\"] # Creating new metrics\n",
        "  cc.fillna(0, inplace=True)\n",
        "\n",
        "  drop_col = [\"MONTHS_BALANCE\", 'AMT_BALANCE', 'AMT_CREDIT_LIMIT_ACTUAL', 'AMT_DRAWINGS_CURRENT',\n",
        "              'AMT_PAYMENT_TOTAL_CURRENT', 'AMT_RECEIVABLE_PRINCIPAL', \"AMT_RECIVABLE\",\n",
        "              'AMT_TOTAL_RECEIVABLE']\n",
        "  drop_col = drop_col + amt_col\n",
        "  cc.drop(drop_col, axis=1, inplace=True)\n",
        "  \n",
        "  # OneHotEncoding\n",
        "  cc, categorical_cols = one_hot_encoder(cc, nan_as_category= False)\n",
        "\n",
        "  # Aggregation\n",
        "  cnt_col = ['CNT_DRAWINGS_ATM_CURRENT', 'CNT_DRAWINGS_CURRENT', 'CNT_DRAWINGS_OTHER_CURRENT',\n",
        "             'CNT_DRAWINGS_POS_CURRENT', 'CNT_INSTALMENT_MATURE_CUM']\n",
        "\n",
        "  # Active status\n",
        "  active = cc[cc['NAME_CONTRACT_STATUS_Active'] == 1].drop(\"SK_ID_PREV\", axis=1)\n",
        "  active_agg = active.groupby('SK_ID_CURR').mean()\n",
        "  active_agg.columns = pd.Index(['ACTIVE_' + e.upper() for e in active_agg.columns.tolist()])\n",
        "  for col in ['ACTIVE_' + e.upper() for e in cnt_col]:\n",
        "    active_agg[col] = active_agg[col].apply(lambda x: round(x))\n",
        "  # Completed status\n",
        "  completed = cc[cc['NAME_CONTRACT_STATUS_Completed'] == 1].drop(\"SK_ID_PREV\", axis=1)\n",
        "  completed_agg = completed.groupby('SK_ID_CURR').mean()\n",
        "  completed_agg.columns = pd.Index(['COMPLETED_' + e.upper() for e in completed_agg.columns.tolist()])\n",
        "  for col in ['COMPLETED_' + e.upper() for e in cnt_col]:\n",
        "    completed_agg[col] = completed_agg[col].apply(lambda x: round(x))\n",
        "  result = completed_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
        "  # Signed status\n",
        "  signed = cc[cc['NAME_CONTRACT_STATUS_Signed'] == 1].drop(\"SK_ID_PREV\", axis=1)\n",
        "  signed_agg = signed.groupby('SK_ID_CURR').mean()\n",
        "  signed_agg.columns = pd.Index(['SIGNED_' + e.upper() for e in signed_agg.columns.tolist()])\n",
        "  for col in ['SIGNED_' + e.upper() for e in cnt_col]:\n",
        "    signed_agg[col] = signed_agg[col].apply(lambda x: round(x))  \n",
        "\n",
        "  result = result.join(signed_agg, how='left', on='SK_ID_CURR').reset_index()\n",
        "\n",
        "  result = result.apply(lambda x: x.replace(np.inf, np.nan) if x.dtype != \"O\" else x, axis=0)\n",
        "\n",
        "  del amt_col, categorical_cols, active, active_agg, completed, completed_agg, signed, signed_agg; gc.collect()\n",
        "  return result"
      ],
      "metadata": {
        "id": "RLW-kg2xbFU2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installment Payment"
      ],
      "metadata": {
        "id": "Da59GkCaliZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_installment_payment(num_rows=None):\n",
        "  ip = pd.read_csv(install_balance, nrows=num_rows)\n",
        "\n",
        "  # Handling NaN\n",
        "  ip = ip.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
        "\n",
        "  # Create new metrics\n",
        "  ip['LATE_PAYMENT'] = -ip['DAYS_ENTRY_PAYMENT'] + ip['DAYS_INSTALMENT']\n",
        "  ip['AMT_PAYMENT_TO_INSTALMENT'] =   ip['AMT_PAYMENT'] / ip['AMT_INSTALMENT']\n",
        "\n",
        "  # Drop columns\n",
        "  drop_cols = [\"DAYS_INSTALMENT\", \"DAYS_ENTRY_PAYMENT\", \"AMT_INSTALMENT\", \"AMT_PAYMENT\"]\n",
        "  ip.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "  # Aggregation\n",
        "  result = ip.drop('SK_ID_PREV', axis=1).groupby('SK_ID_CURR').mean()\n",
        "  for col in ['NUM_INSTALMENT_VERSION', 'NUM_INSTALMENT_NUMBER']:\n",
        "    result[col] = result[col].apply(lambda x: round(x))\n",
        "  result = result.reset_index()\n",
        "  del ip; gc.collect()\n",
        "  return result\n",
        "  "
      ],
      "metadata": {
        "id": "kGg_vONPcgRc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_posh_cash_balance(num_rows=None):\n",
        "  pc = pd.read_csv(pc_balance, nrows=num_rows)\n",
        "\n",
        "  # Handling NaN\n",
        "  pc = pc.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
        "  \n",
        "  # OneHotEncoding\n",
        "  pc, categorical_cols = one_hot_encoder(pc, nan_as_category= False)\n",
        "\n",
        "  # Calculate months_balance \n",
        "#  pc = pc.groupby(['SK_ID_PREV', \"SK_ID_CURR\"])['MONTHS_BALANCE'].size().reset_index().\\\n",
        "#  merge(pc[[\"SK_ID_PREV\", 'CNT_INSTALMENT', 'CNT_INSTALMENT_FUTURE', 'NAME_CONTRACT_STATUS', \"SK_DPD\", \"SK_DPD_DEF\"]], how='left', on='SK_ID_PREV')\n",
        "\n",
        "#  pc.rename(columns = {'MONTHS_BALANCE':'MONTHS_BALANCE_SIZE_POSH_CASH'}, inplace = True)\n",
        "  # Aggregation\n",
        "  pc_size = pc.drop('SK_ID_PREV', axis=1).groupby('SK_ID_CURR')['MONTHS_BALANCE'].size().reset_index()\n",
        "  pc_sum = pc.drop(['SK_ID_PREV', 'MONTHS_BALANCE'], axis=1).groupby('SK_ID_CURR').mean().reset_index()\n",
        "\n",
        "  pc = pc_size.merge(pc_sum, how='left', on='SK_ID_CURR')\n",
        "\n",
        "  del pc_size, pc_sum; gc.collect()\n",
        "  return pc\n",
        "\n"
      ],
      "metadata": {
        "id": "qzRFDHF7k-kx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Previous Applications"
      ],
      "metadata": {
        "id": "en_ir8mcl_PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_previous_app(num_rows=None):\n",
        "  pa = pd.read_csv(prev_app, nrows=num_rows)\n",
        "\n",
        "  # Handling 365.243 values to NaN\n",
        "  pa['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
        "  pa['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
        "  pa['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
        "  pa['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
        "  pa['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
        "\n",
        "  pa = pa.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
        "  pa = pa.apply(lambda x: x.fillna(x.mode()) if x.dtype == \"O\" else x, axis=0)\n",
        "  \n",
        "  # Create new metrics\n",
        "  pa['PREV_APPLICATION_CREDIT_RATIO'] = pa['AMT_APPLICATION'] / pa['AMT_CREDIT']\n",
        "  pa['PREV_APPLICATION_ANNUITY_TO_CREDIT'] = pa['AMT_ANNUITY'] / pa['AMT_CREDIT']\n",
        "  pa['PREV_APPLICATION_DOWN_PAYMENT_TO_CREDIT'] = pa['AMT_DOWN_PAYMENT'] / pa['AMT_CREDIT']\n",
        "\n",
        "  # Interest ratio on previous application (simplified)\n",
        "  total_payment = pa['AMT_ANNUITY'] * pa['CNT_PAYMENT']\n",
        "  pa['SIMPLE_INTERESTS'] = (total_payment/pa['AMT_CREDIT'] - 1)/pa['CNT_PAYMENT']\n",
        "  new_metrics = ['PREV_APPLICATION_CREDIT_RATIO', 'PREV_APPLICATION_ANNUITY_TO_CREDIT', \n",
        "                 'PREV_APPLICATION_DOWN_PAYMENT_TO_CREDIT','SIMPLE_INTERESTS']\n",
        "  # Handling NaN categorical type\n",
        "  pa = pa.apply(lambda x: x.fillna(x.mode()) if x.dtype == \"O\" else x, axis=0)\n",
        "  drop_cols = ['RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED', 'DAYS_FIRST_DRAWING',\n",
        "                'NAME_CASH_LOAN_PURPOSE', 'CODE_REJECT_REASON', 'FLAG_LAST_APPL_PER_CONTRACT',\n",
        "                'NFLAG_LAST_APPL_IN_DAY', 'SELLERPLACE_AREA']\n",
        "  pa.drop(drop_cols, axis=1, inplace=True)\n",
        "  # One-hot encoding\n",
        "  ohe_columns = [\n",
        "        'NAME_CONTRACT_STATUS', 'NAME_CONTRACT_TYPE', 'CHANNEL_TYPE',\n",
        "        'NAME_TYPE_SUITE', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n",
        "        'NAME_PRODUCT_TYPE', 'NAME_CLIENT_TYPE']\n",
        "\n",
        "  pa =pa[[\"SK_ID_CURR\",\"SK_ID_PREV\"] + new_metrics + ohe_columns]\n",
        "\n",
        "  pa, cat_cols = one_hot_encoder(pa, ohe_columns, nan_as_category= False)\n",
        "\n",
        "  # Aggregation\n",
        "  col_list = pa.columns.tolist()\n",
        "  id_list = [\"SK_ID_CURR\",\"SK_ID_PREV\"]\n",
        "  num_list = [col for col in col_list if col not in cat_cols + id_list]\n",
        "                 \n",
        "  #pa = pa[new_metrics + cat_cols + id_list]\n",
        "  prev_agg = pa.drop(\"SK_ID_PREV\", axis=1).groupby('SK_ID_CURR').mean()\n",
        "  prev_agg.columns = pd.Index(['PREV_' + e.upper() for e in prev_agg.columns.tolist()])\n",
        "  # Previous Applications: Approved Applications - only numerical feature\n",
        "  approved = pa[pa['NAME_CONTRACT_STATUS_Approved'] == 1].drop(\"SK_ID_PREV\", axis=1)\n",
        "  approved_agg = approved.groupby('SK_ID_CURR')[num_list].mean()\n",
        "  approved_agg.columns = pd.Index(['APPROVED_' + e.upper() for e in approved_agg.columns.tolist()])\n",
        "  prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
        "  # Previous Applications: Refused Applications - only numerical features\n",
        "  refused = pa[pa['NAME_CONTRACT_STATUS_Refused'] == 1].drop(\"SK_ID_PREV\", axis=1)\n",
        "  refused_agg = refused.groupby('SK_ID_CURR')[num_list].mean()\n",
        "  refused_agg.columns = pd.Index(['REFUSED_' + e.upper() for e in refused_agg.columns.tolist()])\n",
        "  prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR').reset_index()\n",
        "\n",
        "  prev_agg.fillna(0, inplace=True)\n",
        "  del drop_cols, cat_cols, col_list, id_list, num_list, approved, approved_agg; gc.collect()\n",
        "  #prev_agg = reduce_mem_usage(prev_agg)\n",
        "  return prev_agg"
      ],
      "metadata": {
        "id": "jjEwoAuCGBz3"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application"
      ],
      "metadata": {
        "id": "4AcHOUPefjhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def application_train_test(num_rows = None, nan_as_category = False):\n",
        "    df = pd.read_csv(train, nrows= num_rows)\n",
        "    test_df = pd.read_csv(test, nrows= num_rows)\n",
        "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
        "    df = df.append(test_df).reset_index()\n",
        "\n",
        "    # Split type columns\n",
        "    cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    num_cols = [x for x in num_cols if x !='TARGET']\n",
        "\n",
        "    # Handling anomaly\n",
        "    df = df[df['CODE_GENDER'] != 'XNA']\n",
        "    df = df[df['NAME_FAMILY_STATUS'] != \"Unknown\" ]\n",
        "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
        "\n",
        "    # Handling NaN\n",
        "    df[num_cols] = df[num_cols].apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
        "    df[cat_cols] = df[cat_cols].apply(lambda x: x.fillna(x.mode()) if x.dtype == \"O\" else x, axis=0)\n",
        "     \n",
        "    # NAME_HOUSING_TYPE\n",
        "    df[\"NAME_HOUSING_TYPE\"] = np.where(df[\"NAME_HOUSING_TYPE\"].str.contains(\"House / apartment\"),\n",
        "                                       \"House_apartment\", df[\"NAME_HOUSING_TYPE\"])\n",
        "\n",
        "    # FEATURE ENGINEERING\n",
        "    df['NEW_DAYS_EMPLOYED_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['NEW_INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
        "    df['NEW_INCOME_PER_RATIO'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "    df['NEW_ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['NEW_PAYMENT_RATIO'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
        "    df[\"NEW_EXTSOURCE_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "    df[\"NEW_GOODS_CREDIT_RATIO\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n",
        "    df[\"NEW_GOODS_CREDIT_DIFF_RATIO\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) / df[\"AMT_INCOME_TOTAL\"]\n",
        "    \n",
        "\n",
        "    new_metrics = ['NEW_DAYS_EMPLOYED_RATIO', 'NEW_INCOME_CREDIT_RATIO', 'NEW_INCOME_PER_RATIO',\n",
        "                   'NEW_ANNUITY_INCOME_RATIO', 'NEW_PAYMENT_RATIO', \"NEW_EXTSOURCE_MEAN\",\n",
        "                   \"NEW_GOODS_CREDIT_RATIO\", \"NEW_GOODS_CREDIT_DIFF_RATIO\"]\n",
        "    cat_metrics = [\"NAME_HOUSING_TYPE\"]\n",
        "    id_target = ['TARGET', 'SK_ID_CURR']\n",
        "\n",
        "    # One-Hot Encoding\n",
        "    df = df[new_metrics + cat_metrics + id_target ]\n",
        "    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n",
        "    \n",
        "    del test_df; gc.collect()\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "ivUQYLKDfnX-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving to feather"
      ],
      "metadata": {
        "id": "5pWJsPjajYK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = application_train_test()\n",
        "df.to_feather('application_feather')\n",
        "print(df.shape)\n",
        "del df\n",
        "df = pipeline_bureau()\n",
        "df.to_feather('bureau_feather')\n",
        "del df\n",
        "df = pipeline_credit_balance()\n",
        "df.to_feather('credit_balance_feather')\n",
        "del df\n",
        "df = pipeline_posh_cash_balance()\n",
        "df.to_feather('posh_cash_balance_feather')\n",
        "del df\n",
        "df = pipeline_previous_app()\n",
        "df.to_feather('previous_app_feather')\n",
        "del df\n",
        "df = pipeline_installment_payment()\n",
        "df.to_feather('installment_payment_feather')\n",
        "del df"
      ],
      "metadata": {
        "id": "OB82OHuOE8t0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25541679-921c-44d2-936a-1abfe55af1d3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 307511, test samples: 48744\n",
            "(356249, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "U53ft9O0m4B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_feather('application_feather')\n",
        "pos = pd.read_feather('posh_cash_balance_feather')\n",
        "bb = pd.read_feather('bureau_feather')\n",
        "cc = pd.read_feather('credit_balance_feather')\n",
        "ins = pd.read_feather('installment_payment_feather')\n",
        "prev = pd.read_feather('previous_app_feather')\n",
        "\n",
        "print(df.shape, pos.shape, bb.shape, cc.shape, ins.shape, prev.shape)\n",
        "\n",
        "for i in [pos, bb, cc, ins, prev]:\n",
        "    df = pd.merge(df, i , how = \"left\", on = \"SK_ID_CURR\")\n",
        "    \n",
        "print(df.shape)\n",
        "\n",
        "del pos, bb, ins, cc, prev"
      ],
      "metadata": {
        "id": "9cH89nv9nbQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ae9359-2532-4a8c-bb50-eef8ebdf3431"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(356249, 16) (337252, 15) (305811, 38) (6, 46) (339587, 5) (338857, 65)\n",
            "(356249, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
        "df = df.apply(lambda x: x.fillna(x.mode()) if x.dtype == \"O\" else x, axis=0)"
      ],
      "metadata": {
        "id": "MXDucX_AX_AR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "df.columns = list(map(lambda x: str(x).replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"_/_\", \"_\").upper(), df.columns))\n",
        "import re\n",
        "df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "print(df.shape)\n",
        "\n",
        "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "train = df[df.TARGET.isnull() == False]\n",
        "test = df[df.TARGET.isnull()]\n",
        "\n",
        "random_state_val =42\n",
        "test_size_val =0.1\n",
        "train, val = train_test_split(train, test_size = test_size_val, random_state = random_state_val)\n",
        "\n",
        "\n",
        "x_train = train.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\n",
        "x_train = x_train.fillna(0)\n",
        "x_val = val.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\n",
        "x_val = x_val.fillna(0)\n",
        "x_test = test.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\n",
        "x_test = x_test.fillna(0)\n",
        "y_train = train.TARGET\n",
        "y_val = val.TARGET"
      ],
      "metadata": {
        "id": "pUOYCvG_o8A-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b268ffe-664e-4f5e-b8c6-888e1b6d5ad7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(356249, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "x_train, y_train = oversample.fit_resample(x_train, y_train)\n",
        "x_val, y_val = oversample.fit_resample(x_val, y_val)"
      ],
      "metadata": {
        "id": "OJfyGtDDtlPG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model=LogisticRegression(max_iter=1000)\n",
        "model.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAlY1I5QTLS_",
        "outputId": "11f413b4-f444-4d6f-d596-f7178f83571c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_train, model.predict_proba(x_train.values)[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "pred_val = model.predict(x_train)\n",
        "tp, fn, fp, tn = confusion_matrix(y_train, pred_val, labels=[1,0]).ravel()\n",
        "\n",
        "conf_matrix = pd.DataFrame(\n",
        "    confusion_matrix(y_train.values.ravel(), pred_val),\n",
        "    columns=['Predicted Value 0', 'Predicted Value 1'],\n",
        "    index=['True Value 0', 'True Value 1']\n",
        ")\n",
        "\n",
        "print(\"1. Counfusion Matrix\")\n",
        "print(conf_matrix.T)\n",
        "print(\"\")\n",
        "\n",
        "print(\"2. Classification Report\")\n",
        "print(classification_report(y_train.values.ravel(), pred_val))\n",
        "\n",
        "Accuracy_Rate = (tp + tn) / (tp + tn + fp + fn)\n",
        "Recall_Rate = tp / (tp + fn)\n",
        "Precision_Rate = tp / (tp + fp)\n",
        "Specificity_Rate = tn / (tn + fp)\n",
        "F1_Score = (Precision_Rate * Recall_Rate) / (Precision_Rate + Recall_Rate) * 2\n",
        "\n",
        "print(\"3. Model Metric Sumamry\")\n",
        "print(\" - Accuracy Rate    : {:2.3f} %\".format(Accuracy_Rate*100))\n",
        "print(\" - Recall Rate      : {:2.3f} %\".format(Recall_Rate*100))\n",
        "print(\" - Precision Rate   : {:2.3f} %\".format(Precision_Rate*100))\n",
        "print(\" - Specificity Rate : {:2.3f} %\".format(Specificity_Rate*100))\n",
        "print(\" - F1 Score         : {:2.3f} \".format(F1_Score*100))\n",
        "print(\" - ROC AUC          : {:2.3f} \".format(roc_auc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CILPEI-mYUVq",
        "outputId": "016fc83b-170a-49c1-8756-f932cefd4bb8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Counfusion Matrix\n",
            "                   True Value 0  True Value 1\n",
            "Predicted Value 0        193341        152562\n",
            "Predicted Value 1        104976        145755\n",
            "\n",
            "2. Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.65      0.60    298317\n",
            "         1.0       0.58      0.49      0.53    298317\n",
            "\n",
            "    accuracy                           0.57    596634\n",
            "   macro avg       0.57      0.57      0.57    596634\n",
            "weighted avg       0.57      0.57      0.57    596634\n",
            "\n",
            "3. Model Metric Sumamry\n",
            " - Accuracy Rate    : 56.835 %\n",
            " - Recall Rate      : 48.859 %\n",
            " - Precision Rate   : 58.132 %\n",
            " - Specificity Rate : 64.811 %\n",
            " - F1 Score         : 53.094 \n",
            " - ROC AUC          : 59.745 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_val, model.predict_proba(x_val.values)[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "pred_val = model.predict(x_val)\n",
        "tp, fn, fp, tn = confusion_matrix(y_val, pred_val, labels=[1,0]).ravel()\n",
        "\n",
        "conf_matrix = pd.DataFrame(\n",
        "    confusion_matrix(y_val.values.ravel(), pred_val),\n",
        "    columns=['Predicted Value 0', 'Predicted Value 1'],\n",
        "    index=['True Value 0', 'True Value 1']\n",
        ")\n",
        "\n",
        "print(\"1. Counfusion Matrix\")\n",
        "print(conf_matrix.T)\n",
        "print(\"\")\n",
        "\n",
        "print(\"2. Classification Report\")\n",
        "print(classification_report(y_val.values.ravel(), pred_val))\n",
        "\n",
        "Accuracy_Rate = (tp + tn) / (tp + tn + fp + fn)\n",
        "Recall_Rate = tp / (tp + fn)\n",
        "Precision_Rate = tp / (tp + fp)\n",
        "Specificity_Rate = tn / (tn + fp)\n",
        "F1_Score = (Precision_Rate * Recall_Rate) / (Precision_Rate + Recall_Rate) * 2\n",
        "\n",
        "print(\"3. Model Metric Sumamry\")\n",
        "print(\" - Accuracy Rate    : {:2.3f} %\".format(Accuracy_Rate*100))\n",
        "print(\" - Recall Rate      : {:2.3f} %\".format(Recall_Rate*100))\n",
        "print(\" - Precision Rate   : {:2.3f} %\".format(Precision_Rate*100))\n",
        "print(\" - Specificity Rate : {:2.3f} %\".format(Specificity_Rate*100))\n",
        "print(\" - F1 Score         : {:2.3f} \".format(F1_Score*100))\n",
        "print(\" - ROC AUC          : {:2.3f} \".format(roc_auc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uPXVG9YTdYt",
        "outputId": "99bfc043-743a-45fd-d4b2-1a82d4aca9e3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Counfusion Matrix\n",
            "                   True Value 0  True Value 1\n",
            "Predicted Value 0         21254         17620\n",
            "Predicted Value 1         11853         15487\n",
            "\n",
            "2. Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.64      0.59     33107\n",
            "         1.0       0.57      0.47      0.51     33107\n",
            "\n",
            "    accuracy                           0.55     66214\n",
            "   macro avg       0.56      0.55      0.55     66214\n",
            "weighted avg       0.56      0.55      0.55     66214\n",
            "\n",
            "3. Model Metric Sumamry\n",
            " - Accuracy Rate    : 55.488 %\n",
            " - Recall Rate      : 46.779 %\n",
            " - Precision Rate   : 56.646 %\n",
            " - Specificity Rate : 64.198 %\n",
            " - F1 Score         : 51.242 \n",
            " - ROC AUC          : 58.286 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "model = LGBMClassifier(\n",
        "    nthread=4,\n",
        "    n_estimators=10000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=34,\n",
        "    colsample_bytree=0.9497036,\n",
        "    subsample=0.8715623,\n",
        "    max_depth=8,\n",
        "    reg_alpha=0.041545473,\n",
        "    reg_lambda=0.0735294,\n",
        "    min_split_gain=0.0222415,\n",
        "    min_child_weight=39.3259775,\n",
        "    silent=-1,\n",
        "    verbose=-1, )\n",
        "\n",
        "model.fit(x_train, y_train, eval_set=[(x_val, y_val)],\n",
        "        eval_metric='auc', verbose=200)\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldMWN1L_UwKN",
        "outputId": "522e50ee-aedf-49d7-d813-83ddb47e322d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's auc: 0.766106\tvalid_0's binary_logloss: 0.579374\n",
            "[400]\tvalid_0's auc: 0.773475\tvalid_0's binary_logloss: 0.571033\n",
            "[600]\tvalid_0's auc: 0.77582\tvalid_0's binary_logloss: 0.568782\n",
            "[800]\tvalid_0's auc: 0.776755\tvalid_0's binary_logloss: 0.568218\n",
            "[1000]\tvalid_0's auc: 0.77688\tvalid_0's binary_logloss: 0.568471\n",
            "[1200]\tvalid_0's auc: 0.77688\tvalid_0's binary_logloss: 0.569013\n",
            "[1400]\tvalid_0's auc: 0.776445\tvalid_0's binary_logloss: 0.570151\n",
            "[1600]\tvalid_0's auc: 0.776078\tvalid_0's binary_logloss: 0.571322\n",
            "[1800]\tvalid_0's auc: 0.775667\tvalid_0's binary_logloss: 0.572618\n",
            "[2000]\tvalid_0's auc: 0.7752\tvalid_0's binary_logloss: 0.574074\n",
            "[2200]\tvalid_0's auc: 0.774992\tvalid_0's binary_logloss: 0.575481\n",
            "[2400]\tvalid_0's auc: 0.774617\tvalid_0's binary_logloss: 0.57701\n",
            "[2600]\tvalid_0's auc: 0.774287\tvalid_0's binary_logloss: 0.578502\n",
            "[2800]\tvalid_0's auc: 0.774377\tvalid_0's binary_logloss: 0.579761\n",
            "[3000]\tvalid_0's auc: 0.773727\tvalid_0's binary_logloss: 0.58178\n",
            "[3200]\tvalid_0's auc: 0.773369\tvalid_0's binary_logloss: 0.583618\n",
            "[3400]\tvalid_0's auc: 0.772836\tvalid_0's binary_logloss: 0.585721\n",
            "[3600]\tvalid_0's auc: 0.772321\tvalid_0's binary_logloss: 0.587829\n",
            "[3800]\tvalid_0's auc: 0.771853\tvalid_0's binary_logloss: 0.590047\n",
            "[4000]\tvalid_0's auc: 0.771329\tvalid_0's binary_logloss: 0.592332\n",
            "[4200]\tvalid_0's auc: 0.770971\tvalid_0's binary_logloss: 0.594524\n",
            "[4400]\tvalid_0's auc: 0.770444\tvalid_0's binary_logloss: 0.596927\n",
            "[4600]\tvalid_0's auc: 0.769815\tvalid_0's binary_logloss: 0.599487\n",
            "[4800]\tvalid_0's auc: 0.769346\tvalid_0's binary_logloss: 0.601984\n",
            "[5000]\tvalid_0's auc: 0.768776\tvalid_0's binary_logloss: 0.604492\n",
            "[5200]\tvalid_0's auc: 0.768166\tvalid_0's binary_logloss: 0.607107\n",
            "[5400]\tvalid_0's auc: 0.767922\tvalid_0's binary_logloss: 0.609469\n",
            "[5600]\tvalid_0's auc: 0.767248\tvalid_0's binary_logloss: 0.612338\n",
            "[5800]\tvalid_0's auc: 0.766831\tvalid_0's binary_logloss: 0.614801\n",
            "[6000]\tvalid_0's auc: 0.766214\tvalid_0's binary_logloss: 0.617589\n",
            "[6200]\tvalid_0's auc: 0.765722\tvalid_0's binary_logloss: 0.620323\n",
            "[6400]\tvalid_0's auc: 0.765033\tvalid_0's binary_logloss: 0.62339\n",
            "[6600]\tvalid_0's auc: 0.764555\tvalid_0's binary_logloss: 0.62619\n",
            "[6800]\tvalid_0's auc: 0.764338\tvalid_0's binary_logloss: 0.628709\n",
            "[7000]\tvalid_0's auc: 0.763935\tvalid_0's binary_logloss: 0.631628\n",
            "[7200]\tvalid_0's auc: 0.76331\tvalid_0's binary_logloss: 0.634787\n",
            "[7400]\tvalid_0's auc: 0.762915\tvalid_0's binary_logloss: 0.637657\n",
            "[7600]\tvalid_0's auc: 0.762647\tvalid_0's binary_logloss: 0.640495\n",
            "[7800]\tvalid_0's auc: 0.762247\tvalid_0's binary_logloss: 0.643418\n",
            "[8000]\tvalid_0's auc: 0.761755\tvalid_0's binary_logloss: 0.646484\n",
            "[8200]\tvalid_0's auc: 0.761428\tvalid_0's binary_logloss: 0.649277\n",
            "[8400]\tvalid_0's auc: 0.761099\tvalid_0's binary_logloss: 0.652268\n",
            "[8600]\tvalid_0's auc: 0.760696\tvalid_0's binary_logloss: 0.655255\n",
            "[8800]\tvalid_0's auc: 0.760083\tvalid_0's binary_logloss: 0.65873\n",
            "[9000]\tvalid_0's auc: 0.759565\tvalid_0's binary_logloss: 0.661919\n",
            "[9200]\tvalid_0's auc: 0.759054\tvalid_0's binary_logloss: 0.665228\n",
            "[9400]\tvalid_0's auc: 0.758768\tvalid_0's binary_logloss: 0.668098\n",
            "[9600]\tvalid_0's auc: 0.75843\tvalid_0's binary_logloss: 0.670995\n",
            "[9800]\tvalid_0's auc: 0.757852\tvalid_0's binary_logloss: 0.674455\n",
            "[10000]\tvalid_0's auc: 0.757642\tvalid_0's binary_logloss: 0.677211\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier(colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8,\n",
              "               min_child_weight=39.3259775, min_split_gain=0.0222415,\n",
              "               n_estimators=10000, nthread=4, num_leaves=34,\n",
              "               reg_alpha=0.041545473, reg_lambda=0.0735294, silent=-1,\n",
              "               subsample=0.8715623, verbose=-1)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_train, model.predict_proba(x_train.values)[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "pred_val = model.predict(x_train)\n",
        "tp, fn, fp, tn = confusion_matrix(y_train, pred_val, labels=[1,0]).ravel()\n",
        "\n",
        "conf_matrix = pd.DataFrame(\n",
        "    confusion_matrix(y_train.values.ravel(), pred_val),\n",
        "    columns=['Predicted Value 0', 'Predicted Value 1'],\n",
        "    index=['True Value 0', 'True Value 1']\n",
        ")\n",
        "\n",
        "print(\"1. Counfusion Matrix\")\n",
        "print(conf_matrix.T)\n",
        "print(\"\")\n",
        "\n",
        "print(\"2. Classification Report\")\n",
        "print(classification_report(y_train.values.ravel(), pred_val))\n",
        "\n",
        "Accuracy_Rate = (tp + tn) / (tp + tn + fp + fn)\n",
        "Recall_Rate = tp / (tp + fn)\n",
        "Precision_Rate = tp / (tp + fp)\n",
        "Specificity_Rate = tn / (tn + fp)\n",
        "F1_Score = (Precision_Rate * Recall_Rate) / (Precision_Rate + Recall_Rate) * 2\n",
        "\n",
        "print(\"3. Model Metric Sumamry\")\n",
        "print(\" - Accuracy Rate    : {:2.3f} %\".format(Accuracy_Rate*100))\n",
        "print(\" - Recall Rate      : {:2.3f} %\".format(Recall_Rate*100))\n",
        "print(\" - Precision Rate   : {:2.3f} %\".format(Precision_Rate*100))\n",
        "print(\" - Specificity Rate : {:2.3f} %\".format(Specificity_Rate*100))\n",
        "print(\" - F1 Score         : {:2.3f} \".format(F1_Score*100))\n",
        "print(\" - ROC AUC          : {:2.3f} \".format(roc_auc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7wtPzCnYP3l",
        "outputId": "b8f4a66e-98ff-4454-eb33-c21c77c8841d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Counfusion Matrix\n",
            "                   True Value 0  True Value 1\n",
            "Predicted Value 0        271476          8471\n",
            "Predicted Value 1         26841        289846\n",
            "\n",
            "2. Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.91      0.94    298317\n",
            "         1.0       0.92      0.97      0.94    298317\n",
            "\n",
            "    accuracy                           0.94    596634\n",
            "   macro avg       0.94      0.94      0.94    596634\n",
            "weighted avg       0.94      0.94      0.94    596634\n",
            "\n",
            "3. Model Metric Sumamry\n",
            " - Accuracy Rate    : 94.081 %\n",
            " - Recall Rate      : 97.160 %\n",
            " - Precision Rate   : 91.524 %\n",
            " - Specificity Rate : 91.003 %\n",
            " - F1 Score         : 94.258 \n",
            " - ROC AUC          : 98.621 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_val, model.predict_proba(x_val.values)[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "pred_val = model.predict(x_val)\n",
        "tp, fn, fp, tn = confusion_matrix(y_val, pred_val, labels=[1,0]).ravel()\n",
        "\n",
        "conf_matrix = pd.DataFrame(\n",
        "    confusion_matrix(y_val.values.ravel(), pred_val),\n",
        "    columns=['Predicted Value 0', 'Predicted Value 1'],\n",
        "    index=['True Value 0', 'True Value 1']\n",
        ")\n",
        "\n",
        "print(\"1. Counfusion Matrix\")\n",
        "print(conf_matrix.T)\n",
        "print(\"\")\n",
        "\n",
        "print(\"2. Classification Report\")\n",
        "print(classification_report(y_val.values.ravel(), pred_val))\n",
        "\n",
        "Accuracy_Rate = (tp + tn) / (tp + tn + fp + fn)\n",
        "Recall_Rate = tp / (tp + fn)\n",
        "Precision_Rate = tp / (tp + fp)\n",
        "Specificity_Rate = tn / (tn + fp)\n",
        "F1_Score = (Precision_Rate * Recall_Rate) / (Precision_Rate + Recall_Rate) * 2\n",
        "\n",
        "print(\"3. Model Metric Sumamry\")\n",
        "print(\" - Accuracy Rate    : {:2.3f} %\".format(Accuracy_Rate*100))\n",
        "print(\" - Recall Rate      : {:2.3f} %\".format(Recall_Rate*100))\n",
        "print(\" - Precision Rate   : {:2.3f} %\".format(Precision_Rate*100))\n",
        "print(\" - Specificity Rate : {:2.3f} %\".format(Specificity_Rate*100))\n",
        "print(\" - F1 Score         : {:2.3f} \".format(F1_Score*100))\n",
        "print(\" - ROC AUC          : {:2.3f} \".format(roc_auc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjTyqMMkU3WX",
        "outputId": "a20a48d7-8942-4ba8-d4b0-a3378382edda"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Counfusion Matrix\n",
            "                   True Value 0  True Value 1\n",
            "Predicted Value 0         29083         18983\n",
            "Predicted Value 1          4024         14124\n",
            "\n",
            "2. Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.88      0.72     33107\n",
            "         1.0       0.78      0.43      0.55     33107\n",
            "\n",
            "    accuracy                           0.65     66214\n",
            "   macro avg       0.69      0.65      0.63     66214\n",
            "weighted avg       0.69      0.65      0.63     66214\n",
            "\n",
            "3. Model Metric Sumamry\n",
            " - Accuracy Rate    : 65.254 %\n",
            " - Recall Rate      : 42.662 %\n",
            " - Precision Rate   : 77.827 %\n",
            " - Specificity Rate : 87.845 %\n",
            " - F1 Score         : 55.113 \n",
            " - ROC AUC          : 75.764 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save best model"
      ],
      "metadata": {
        "id": "wnqwAZKkTUpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# LightGBM parameters found by Bayesian optimization\n",
        "clf = LGBMClassifier(\n",
        "    nthread=4,\n",
        "    n_estimators=10000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=34,\n",
        "    colsample_bytree=0.9497036,\n",
        "    subsample=0.8715623,\n",
        "    max_depth=8,\n",
        "    reg_alpha=0.041545473,\n",
        "    reg_lambda=0.0735294,\n",
        "    min_split_gain=0.0222415,\n",
        "    min_child_weight=39.3259775,\n",
        "    silent=-1,\n",
        "    verbose=-1, )\n",
        "\n",
        "clf.fit(x_train, y_train, eval_set=[(x_val, y_val)],\n",
        "        eval_metric='auc', verbose=200)"
      ],
      "metadata": {
        "id": "5lSBZ4UnpCIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d004336-cd33-4ecc-d65b-092c97ce84e7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's auc: 0.766106\tvalid_0's binary_logloss: 0.579374\n",
            "[400]\tvalid_0's auc: 0.773475\tvalid_0's binary_logloss: 0.571033\n",
            "[600]\tvalid_0's auc: 0.77582\tvalid_0's binary_logloss: 0.568782\n",
            "[800]\tvalid_0's auc: 0.776755\tvalid_0's binary_logloss: 0.568218\n",
            "[1000]\tvalid_0's auc: 0.77688\tvalid_0's binary_logloss: 0.568471\n",
            "[1200]\tvalid_0's auc: 0.77688\tvalid_0's binary_logloss: 0.569013\n",
            "[1400]\tvalid_0's auc: 0.776445\tvalid_0's binary_logloss: 0.570151\n",
            "[1600]\tvalid_0's auc: 0.776078\tvalid_0's binary_logloss: 0.571322\n",
            "[1800]\tvalid_0's auc: 0.775667\tvalid_0's binary_logloss: 0.572618\n",
            "[2000]\tvalid_0's auc: 0.7752\tvalid_0's binary_logloss: 0.574074\n",
            "[2200]\tvalid_0's auc: 0.774992\tvalid_0's binary_logloss: 0.575481\n",
            "[2400]\tvalid_0's auc: 0.774617\tvalid_0's binary_logloss: 0.57701\n",
            "[2600]\tvalid_0's auc: 0.774287\tvalid_0's binary_logloss: 0.578502\n",
            "[2800]\tvalid_0's auc: 0.774377\tvalid_0's binary_logloss: 0.579761\n",
            "[3000]\tvalid_0's auc: 0.773727\tvalid_0's binary_logloss: 0.58178\n",
            "[3200]\tvalid_0's auc: 0.773369\tvalid_0's binary_logloss: 0.583618\n",
            "[3400]\tvalid_0's auc: 0.772836\tvalid_0's binary_logloss: 0.585721\n",
            "[3600]\tvalid_0's auc: 0.772321\tvalid_0's binary_logloss: 0.587829\n",
            "[3800]\tvalid_0's auc: 0.771853\tvalid_0's binary_logloss: 0.590047\n",
            "[4000]\tvalid_0's auc: 0.771329\tvalid_0's binary_logloss: 0.592332\n",
            "[4200]\tvalid_0's auc: 0.770971\tvalid_0's binary_logloss: 0.594524\n",
            "[4400]\tvalid_0's auc: 0.770444\tvalid_0's binary_logloss: 0.596927\n",
            "[4600]\tvalid_0's auc: 0.769815\tvalid_0's binary_logloss: 0.599487\n",
            "[4800]\tvalid_0's auc: 0.769346\tvalid_0's binary_logloss: 0.601984\n",
            "[5000]\tvalid_0's auc: 0.768776\tvalid_0's binary_logloss: 0.604492\n",
            "[5200]\tvalid_0's auc: 0.768166\tvalid_0's binary_logloss: 0.607107\n",
            "[5400]\tvalid_0's auc: 0.767922\tvalid_0's binary_logloss: 0.609469\n",
            "[5600]\tvalid_0's auc: 0.767248\tvalid_0's binary_logloss: 0.612338\n",
            "[5800]\tvalid_0's auc: 0.766831\tvalid_0's binary_logloss: 0.614801\n",
            "[6000]\tvalid_0's auc: 0.766214\tvalid_0's binary_logloss: 0.617589\n",
            "[6200]\tvalid_0's auc: 0.765722\tvalid_0's binary_logloss: 0.620323\n",
            "[6400]\tvalid_0's auc: 0.765033\tvalid_0's binary_logloss: 0.62339\n",
            "[6600]\tvalid_0's auc: 0.764555\tvalid_0's binary_logloss: 0.62619\n",
            "[6800]\tvalid_0's auc: 0.764338\tvalid_0's binary_logloss: 0.628709\n",
            "[7000]\tvalid_0's auc: 0.763935\tvalid_0's binary_logloss: 0.631628\n",
            "[7200]\tvalid_0's auc: 0.76331\tvalid_0's binary_logloss: 0.634787\n",
            "[7400]\tvalid_0's auc: 0.762915\tvalid_0's binary_logloss: 0.637657\n",
            "[7600]\tvalid_0's auc: 0.762647\tvalid_0's binary_logloss: 0.640495\n",
            "[7800]\tvalid_0's auc: 0.762247\tvalid_0's binary_logloss: 0.643418\n",
            "[8000]\tvalid_0's auc: 0.761755\tvalid_0's binary_logloss: 0.646484\n",
            "[8200]\tvalid_0's auc: 0.761428\tvalid_0's binary_logloss: 0.649277\n",
            "[8400]\tvalid_0's auc: 0.761099\tvalid_0's binary_logloss: 0.652268\n",
            "[8600]\tvalid_0's auc: 0.760696\tvalid_0's binary_logloss: 0.655255\n",
            "[8800]\tvalid_0's auc: 0.760083\tvalid_0's binary_logloss: 0.65873\n",
            "[9000]\tvalid_0's auc: 0.759565\tvalid_0's binary_logloss: 0.661919\n",
            "[9200]\tvalid_0's auc: 0.759054\tvalid_0's binary_logloss: 0.665228\n",
            "[9400]\tvalid_0's auc: 0.758768\tvalid_0's binary_logloss: 0.668098\n",
            "[9600]\tvalid_0's auc: 0.75843\tvalid_0's binary_logloss: 0.670995\n",
            "[9800]\tvalid_0's auc: 0.757852\tvalid_0's binary_logloss: 0.674455\n",
            "[10000]\tvalid_0's auc: 0.757642\tvalid_0's binary_logloss: 0.677211\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier(colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8,\n",
              "               min_child_weight=39.3259775, min_split_gain=0.0222415,\n",
              "               n_estimators=10000, nthread=4, num_leaves=34,\n",
              "               reg_alpha=0.041545473, reg_lambda=0.0735294, silent=-1,\n",
              "               subsample=0.8715623, verbose=-1)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LGBMClassifier(colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8,\n",
        "               min_child_weight=39.3259775, min_split_gain=0.0222415,\n",
        "               n_estimators=10000, nthread=4, num_leaves=34,\n",
        "               reg_alpha=0.041545473, reg_lambda=0.0735294, silent=-1,\n",
        "               subsample=0.8715623, verbose=-1)"
      ],
      "metadata": {
        "id": "22PJAD9v-QaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(clf, open('saved_model.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "OIQx7ekO6iyW"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, _ = roc_curve(y_train, clf.predict_proba(x_train.values)[:, 1])\n",
        "roc_auc = auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "jukRk4s3-l6S"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred_val = clf.predict(x_train)\n",
        "tp, fn, fp, tn = confusion_matrix(y_train, pred_val, labels=[1,0]).ravel()"
      ],
      "metadata": {
        "id": "6IG6zDUl-Cb6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = pd.DataFrame(\n",
        "    confusion_matrix(y_train.values.ravel(), pred_val),\n",
        "    columns=['Predicted Value 0', 'Predicted Value 1'],\n",
        "    index=['True Value 0', 'True Value 1']\n",
        ")\n",
        "\n",
        "print(\"1. Counfusion Matrix\")\n",
        "print(conf_matrix.T)\n",
        "print(\"\")\n",
        "\n",
        "print(\"2. Classification Report\")\n",
        "print(classification_report(y_train.values.ravel(), pred_val))\n",
        "\n",
        "Accuracy_Rate = (tp + tn) / (tp + tn + fp + fn)\n",
        "Recall_Rate = tp / (tp + fn)\n",
        "Precision_Rate = tp / (tp + fp)\n",
        "Specificity_Rate = tn / (tn + fp)\n",
        "F1_Score = (Precision_Rate * Recall_Rate) / (Precision_Rate + Recall_Rate) * 2\n",
        "\n",
        "print(\"3. Model Metric Sumamry\")\n",
        "print(\" - Accuracy Rate    : {:2.3f} %\".format(Accuracy_Rate*100))\n",
        "print(\" - Recall Rate      : {:2.3f} %\".format(Recall_Rate*100))\n",
        "print(\" - Precision Rate   : {:2.3f} %\".format(Precision_Rate*100))\n",
        "print(\" - Specificity Rate : {:2.3f} %\".format(Specificity_Rate*100))\n",
        "print(\" - F1 Score         : {:2.3f} \".format(F1_Score*100))\n",
        "print(\" - ROC AUC          : {:2.3f} \".format(roc_auc*100))"
      ],
      "metadata": {
        "id": "y2ZGdDrT-AQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f552dc47-3a31-40e4-a036-4c3030b85b83"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Counfusion Matrix\n",
            "                   True Value 0  True Value 1\n",
            "Predicted Value 0        271476          8471\n",
            "Predicted Value 1         26841        289846\n",
            "\n",
            "2. Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.91      0.94    298317\n",
            "         1.0       0.92      0.97      0.94    298317\n",
            "\n",
            "    accuracy                           0.94    596634\n",
            "   macro avg       0.94      0.94      0.94    596634\n",
            "weighted avg       0.94      0.94      0.94    596634\n",
            "\n",
            "3. Model Metric Sumamry\n",
            " - Accuracy Rate    : 94.081 %\n",
            " - Recall Rate      : 97.160 %\n",
            " - Precision Rate   : 91.524 %\n",
            " - Specificity Rate : 91.003 %\n",
            " - F1 Score         : 94.258 \n",
            " - ROC AUC          : 98.621 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try"
      ],
      "metadata": {
        "id": "Q1lqD1N9XJ6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AGGREGATIONS\n",
        "\n",
        "BUREAU_AGG = {\n",
        "    'SK_ID_BUREAU': ['nunique'],\n",
        "    'DAYS_CREDIT': ['min', 'max', 'mean'],\n",
        "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
        "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
        "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
        "    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
        "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
        "    'AMT_ANNUITY': ['mean'],\n",
        "    'DEBT_CREDIT_DIFF': ['mean', 'sum'],\n",
        "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
        "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
        "    # Categorical\n",
        "    'STATUS_0': ['mean'],\n",
        "    'STATUS_1': ['mean'],\n",
        "    'STATUS_12345': ['mean'],\n",
        "    'STATUS_C': ['mean'],\n",
        "    'STATUS_X': ['mean'],\n",
        "    'CREDIT_ACTIVE_Active': ['mean'],\n",
        "    'CREDIT_ACTIVE_Closed': ['mean'],\n",
        "    'CREDIT_ACTIVE_Sold': ['mean'],\n",
        "    'CREDIT_TYPE_Consumer credit': ['mean'],\n",
        "    'CREDIT_TYPE_Credit card': ['mean'],\n",
        "    'CREDIT_TYPE_Car loan': ['mean'],\n",
        "    'CREDIT_TYPE_Mortgage': ['mean'],\n",
        "    'CREDIT_TYPE_Microloan': ['mean'],\n",
        "    # Group by loan duration features (months)\n",
        "    'LL_AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
        "    'LL_DEBT_CREDIT_DIFF': ['mean'],\n",
        "    'LL_STATUS_12345': ['mean'],\n",
        "}\n",
        "\n",
        "BUREAU_ACTIVE_AGG = {\n",
        "    'DAYS_CREDIT': ['max', 'mean'],\n",
        "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
        "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
        "    'AMT_CREDIT_SUM': ['max', 'sum'],\n",
        "    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n",
        "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean'],\n",
        "    'DAYS_CREDIT_UPDATE': ['min', 'mean'],\n",
        "    'DEBT_PERCENTAGE': ['mean'],\n",
        "    'DEBT_CREDIT_DIFF': ['mean'],\n",
        "    'CREDIT_TO_ANNUITY_RATIO': ['mean'],\n",
        "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
        "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
        "}\n",
        "\n",
        "BUREAU_CLOSED_AGG = {\n",
        "    'DAYS_CREDIT': ['max', 'var'],\n",
        "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
        "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
        "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
        "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
        "    'AMT_CREDIT_SUM_DEBT': ['max', 'sum'],\n",
        "    'DAYS_CREDIT_UPDATE': ['max'],\n",
        "    'ENDDATE_DIF': ['mean'],\n",
        "    'STATUS_12345': ['mean'],\n",
        "}\n",
        "\n",
        "BUREAU_LOAN_TYPE_AGG = {\n",
        "    'DAYS_CREDIT': ['mean', 'max'],\n",
        "    'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n",
        "    'AMT_CREDIT_SUM': ['mean', 'max'],\n",
        "    'AMT_CREDIT_SUM_DEBT': ['mean', 'max'],\n",
        "    'DEBT_PERCENTAGE': ['mean'],\n",
        "    'DEBT_CREDIT_DIFF': ['mean'],\n",
        "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
        "}\n",
        "\n",
        "BUREAU_TIME_AGG = {\n",
        "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
        "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
        "    'AMT_CREDIT_SUM': ['max', 'sum'],\n",
        "    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n",
        "    'DEBT_PERCENTAGE': ['mean'],\n",
        "    'DEBT_CREDIT_DIFF': ['mean'],\n",
        "    'STATUS_0': ['mean'],\n",
        "    'STATUS_12345': ['mean'],\n",
        "}\n",
        "\n",
        "PREVIOUS_AGG = {\n",
        "    'SK_ID_PREV': ['nunique'],\n",
        "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
        "    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n",
        "    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
        "    'RATE_DOWN_PAYMENT': ['max', 'mean'],\n",
        "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "    'CNT_PAYMENT': ['max', 'mean'],\n",
        "    'DAYS_TERMINATION': ['max'],\n",
        "    # Engineered features\n",
        "    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
        "    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean'],\n",
        "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean', 'var'],\n",
        "    'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
        "}\n",
        "\n",
        "PREVIOUS_ACTIVE_AGG = {\n",
        "    'SK_ID_PREV': ['nunique'],\n",
        "    'SIMPLE_INTERESTS': ['mean'],\n",
        "    'AMT_ANNUITY': ['max', 'sum'],\n",
        "    'AMT_APPLICATION': ['max', 'mean'],\n",
        "    'AMT_CREDIT': ['sum'],\n",
        "    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n",
        "    'DAYS_DECISION': ['min', 'mean'],\n",
        "    'CNT_PAYMENT': ['mean', 'sum'],\n",
        "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
        "    # Engineered features\n",
        "    'AMT_PAYMENT': ['sum'],\n",
        "    'INSTALMENT_PAYMENT_DIFF': ['mean', 'max'],\n",
        "    'REMAINING_DEBT': ['max', 'mean', 'sum'],\n",
        "    'REPAYMENT_RATIO': ['mean'],\n",
        "}\n",
        "\n",
        "PREVIOUS_APPROVED_AGG = {\n",
        "    'SK_ID_PREV': ['nunique'],\n",
        "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
        "    'AMT_CREDIT': ['min', 'max', 'mean'],\n",
        "    'AMT_DOWN_PAYMENT': ['max'],\n",
        "    'AMT_GOODS_PRICE': ['max'],\n",
        "    'HOUR_APPR_PROCESS_START': ['min', 'max'],\n",
        "    'DAYS_DECISION': ['min', 'mean'],\n",
        "    'CNT_PAYMENT': ['max', 'mean'],\n",
        "    'DAYS_TERMINATION': ['mean'],\n",
        "    # Engineered features\n",
        "    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
        "    'APPLICATION_CREDIT_DIFF': ['max'],\n",
        "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
        "    # The following features are only for approved applications\n",
        "    'DAYS_FIRST_DRAWING': ['max', 'mean'],\n",
        "    'DAYS_FIRST_DUE': ['min', 'mean'],\n",
        "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
        "    'DAYS_LAST_DUE': ['max', 'mean'],\n",
        "    'DAYS_LAST_DUE_DIFF': ['min', 'max', 'mean'],\n",
        "    'SIMPLE_INTERESTS': ['min', 'max', 'mean'],\n",
        "}\n",
        "\n",
        "PREVIOUS_REFUSED_AGG = {\n",
        "    'AMT_APPLICATION': ['max', 'mean'],\n",
        "    'AMT_CREDIT': ['min', 'max'],\n",
        "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "    'CNT_PAYMENT': ['max', 'mean'],\n",
        "    # Engineered features\n",
        "    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'var'],\n",
        "    'APPLICATION_CREDIT_RATIO': ['min', 'mean'],\n",
        "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
        "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
        "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
        "}\n",
        "\n",
        "PREVIOUS_LATE_PAYMENTS_AGG = {\n",
        "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
        "    # Engineered features\n",
        "    'APPLICATION_CREDIT_DIFF': ['min'],\n",
        "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
        "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
        "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
        "}\n",
        "\n",
        "PREVIOUS_LOAN_TYPE_AGG = {\n",
        "    'AMT_CREDIT': ['sum'],\n",
        "    'AMT_ANNUITY': ['mean', 'max'],\n",
        "    'SIMPLE_INTERESTS': ['min', 'mean', 'max', 'var'],\n",
        "    'APPLICATION_CREDIT_DIFF': ['min', 'var'],\n",
        "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
        "    'DAYS_DECISION': ['max'],\n",
        "    'DAYS_LAST_DUE_1ST_VERSION': ['max', 'mean'],\n",
        "    'CNT_PAYMENT': ['mean'],\n",
        "}\n",
        "\n",
        "PREVIOUS_TIME_AGG = {\n",
        "    'AMT_CREDIT': ['sum'],\n",
        "    'AMT_ANNUITY': ['mean', 'max'],\n",
        "    'SIMPLE_INTERESTS': ['mean', 'max'],\n",
        "    'DAYS_DECISION': ['min', 'mean'],\n",
        "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
        "    # Engineered features\n",
        "    'APPLICATION_CREDIT_DIFF': ['min'],\n",
        "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
        "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
        "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
        "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
        "}\n",
        "\n",
        "POS_CASH_AGG = {\n",
        "    'SK_ID_PREV': ['nunique'],\n",
        "    'MONTHS_BALANCE': ['min', 'max', 'size'],\n",
        "    'SK_DPD': ['max', 'mean', 'sum', 'var'],\n",
        "    'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
        "    'LATE_PAYMENT': ['mean']\n",
        "}\n",
        "\n",
        "INSTALLMENTS_AGG = {\n",
        "    'SK_ID_PREV': ['size', 'nunique'],\n",
        "    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n",
        "    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
        "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
        "    'DPD': ['max', 'mean', 'var'],\n",
        "    'DBD': ['max', 'mean', 'var'],\n",
        "    'PAYMENT_DIFFERENCE': ['mean'],\n",
        "    'PAYMENT_RATIO': ['mean'],\n",
        "    'LATE_PAYMENT': ['mean', 'sum'],\n",
        "    'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n",
        "    'LATE_PAYMENT_RATIO': ['mean'],\n",
        "    'DPD_7': ['mean'],\n",
        "    'DPD_15': ['mean'],\n",
        "    'PAID_OVER': ['mean']\n",
        "}\n",
        "\n",
        "INSTALLMENTS_TIME_AGG = {\n",
        "    'SK_ID_PREV': ['size'],\n",
        "    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n",
        "    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
        "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
        "    'DPD': ['max', 'mean', 'var'],\n",
        "    'DBD': ['max', 'mean', 'var'],\n",
        "    'PAYMENT_DIFFERENCE': ['mean'],\n",
        "    'PAYMENT_RATIO': ['mean'],\n",
        "    'LATE_PAYMENT': ['mean'],\n",
        "    'SIGNIFICANT_LATE_PAYMENT': ['mean'],\n",
        "    'LATE_PAYMENT_RATIO': ['mean'],\n",
        "    'DPD_7': ['mean'],\n",
        "    'DPD_15': ['mean'],\n",
        "}\n",
        "\n",
        "CREDIT_CARD_AGG = {\n",
        "    'MONTHS_BALANCE': ['min'],\n",
        "    'AMT_BALANCE': ['max'],\n",
        "    'AMT_CREDIT_LIMIT_ACTUAL': ['max'],\n",
        "    'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n",
        "    'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n",
        "    'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n",
        "    'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
        "    'AMT_PAYMENT_TOTAL_CURRENT': ['max', 'mean', 'sum', 'var'],\n",
        "    'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
        "    'CNT_DRAWINGS_ATM_CURRENT': ['max', 'mean', 'sum'],\n",
        "    'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n",
        "    'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n",
        "    'SK_DPD': ['mean', 'max', 'sum'],\n",
        "    'SK_DPD_DEF': ['max', 'sum'],\n",
        "    'LIMIT_USE': ['max', 'mean'],\n",
        "    'PAYMENT_DIV_MIN': ['min', 'mean'],\n",
        "    'LATE_PAYMENT': ['max', 'sum'],\n",
        "}\n",
        "\n",
        "CREDIT_CARD_TIME_AGG = {\n",
        "    'CNT_DRAWINGS_ATM_CURRENT': ['mean'],\n",
        "    'SK_DPD': ['max', 'sum'],\n",
        "    'AMT_BALANCE': ['mean', 'max'],\n",
        "    'LIMIT_USE': ['max', 'mean']\n",
        "}\n",
        "\n",
        "def get_bureau_balance(path, num_rows= None):\n",
        "    bb = pd.read_csv(path, nrows= num_rows)\n",
        "    bb, categorical_cols = one_hot_encoder(bb, nan_as_category= False)\n",
        "    \n",
        "    # Calculate rate for each category with decay\n",
        "    bb_processed = bb.groupby('SK_ID_BUREAU')[categorical_cols + ['MONTHS_BALANCE']].mean().reset_index()\n",
        "    \n",
        "    # Min, Max, Count and mean duration of payments (months)\n",
        "    agg = {'MONTHS_BALANCE': ['min', 'max', 'mean', 'size']}\n",
        "    bb_processed = group_and_merge(bb, bb_processed, '', agg, 'SK_ID_BUREAU')\n",
        "    del bb; gc.collect()\n",
        "    return bb_processed\n",
        "\n",
        "def get_bureau(path, num_rows= None):\n",
        "    \"\"\" Process bureau.csv and bureau_balance.csv and return a pandas dataframe. \"\"\"\n",
        "    bureau = pd.read_csv(path, nrows= num_rows)\n",
        "    \n",
        "    # Credit duration and credit/account end date difference\n",
        "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
        "    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
        "    \n",
        "    # Credit to debt ratio and difference\n",
        "    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
        "    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
        "    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_ANNUITY']\n",
        "\n",
        "    # One-hot encoder\n",
        "    bureau, categorical_cols = one_hot_encoder(bureau, nan_as_category= False)\n",
        "    \n",
        "    # Join bureau balance features\n",
        "    bureau = bureau.merge(get_bureau_balance(bureau_balance, num_rows), how='left', on='SK_ID_BUREAU')\n",
        "    \n",
        "    # Flag months with late payments (days past due)\n",
        "    bureau['STATUS_12345'] = 0\n",
        "    for i in range(1,6):\n",
        "        bureau['STATUS_12345'] += bureau['STATUS_{}'.format(i)]\n",
        "\n",
        "    # Aggregate by number of months in balance and merge with bureau (loan length agg)\n",
        "    features = ['AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM',\n",
        "        'AMT_CREDIT_SUM_DEBT', 'DEBT_PERCENTAGE', 'DEBT_CREDIT_DIFF', 'STATUS_0', 'STATUS_12345']\n",
        "    agg_length = bureau.groupby('MONTHS_BALANCE_SIZE')[features].mean().reset_index()\n",
        "    agg_length.rename({feat: 'LL_' + feat for feat in features}, axis=1, inplace=True)\n",
        "    bureau = bureau.merge(agg_length, how='left', on='MONTHS_BALANCE_SIZE')\n",
        "    del agg_length; gc.collect()\n",
        "\n",
        "    # General loans aggregations\n",
        "    agg_bureau = group(bureau, 'BUREAU_', BUREAU_AGG)\n",
        "    \n",
        "    # Active and closed loans aggregations\n",
        "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
        "    agg_bureau = group_and_merge(active,agg_bureau,'BUREAU_ACTIVE_',BUREAU_ACTIVE_AGG)\n",
        "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
        "    agg_bureau = group_and_merge(closed,agg_bureau,'BUREAU_CLOSED_',BUREAU_CLOSED_AGG)\n",
        "    del active, closed; gc.collect()\n",
        "    \n",
        "    # Aggregations for the main loan types\n",
        "    for credit_type in ['Consumer credit', 'Credit card', 'Mortgage', 'Car loan', 'Microloan']:\n",
        "        type_df = bureau[bureau['CREDIT_TYPE_' + credit_type] == 1]\n",
        "        prefix = 'BUREAU_' + credit_type.split(' ')[0].upper() + '_'\n",
        "        agg_bureau = group_and_merge(type_df, agg_bureau, prefix, BUREAU_LOAN_TYPE_AGG)\n",
        "        del type_df; gc.collect()\n",
        "    \n",
        "    # Time based aggregations: last x months\n",
        "    for time_frame in [6, 12]:\n",
        "        prefix = \"BUREAU_LAST{}M_\".format(time_frame)\n",
        "        time_frame_df = bureau[bureau['DAYS_CREDIT'] >= -30*time_frame]\n",
        "        agg_bureau = group_and_merge(time_frame_df, agg_bureau, prefix, BUREAU_TIME_AGG)\n",
        "        del time_frame_df; gc.collect()\n",
        "\n",
        "    # Last loan max overdue\n",
        "    sort_bureau = bureau.sort_values(by=['DAYS_CREDIT'])\n",
        "    gr = sort_bureau.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].last().reset_index()\n",
        "    gr.rename({'AMT_CREDIT_MAX_OVERDUE': 'BUREAU_LAST_LOAN_MAX_OVERDUE'}, inplace=True)\n",
        "    agg_bureau = agg_bureau.merge(gr, on='SK_ID_CURR', how='left')\n",
        "    \n",
        "    # Ratios: total debt/total credit and active loans debt/ active loans credit\n",
        "    agg_bureau['BUREAU_DEBT_OVER_CREDIT'] = \\\n",
        "        agg_bureau['BUREAU_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_AMT_CREDIT_SUM_SUM']\n",
        "    agg_bureau['BUREAU_ACTIVE_DEBT_OVER_CREDIT'] = \\\n",
        "        agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM']\n",
        "    return agg_bureau"
      ],
      "metadata": {
        "id": "zTaCG4FjKTIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_bureau_balance(bureau_balance)"
      ],
      "metadata": {
        "id": "aZP58WepXo4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_bureau(bureau)"
      ],
      "metadata": {
        "id": "ri8QPxkbV4tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}